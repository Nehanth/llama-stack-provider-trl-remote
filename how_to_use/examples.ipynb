{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# How to Use Llama Stack Provider TRL\n",
    "\n",
    "This notebook demonstrates how to interact with the Llama Stack Provider TRL (Transformers Reinforcement Learning) for post-training tasks, specifically Direct Preference Optimization (DPO).\n",
    "\n",
    "## Overview\n",
    "\n",
    "The TRL provider enables you to:\n",
    "- Upload preference datasets for training\n",
    "- Run DPO (Direct Preference Optimization) training jobs\n",
    "- Monitor training progress and retrieve artifacts\n",
    "- Manage post-training workflows through a REST API\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure that:\n",
    "1. The Llama Stack TRL provider server is running on `http://127.0.0.1:8321`\n",
    "2. You have the required dependencies installed (`requests`)\n",
    "3. The server has the necessary providers configured (TRL for post-training, localfs for datasets)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by importing the required libraries and setting up our API configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "\n",
    "# Base URL for the Llama Stack TRL provider API\n",
    "# Make sure the server is running on this address and port\n",
    "base_url = \"http://127.0.0.1:8321\"\n",
    "\n",
    "# Headers for GET requests (retrieving data)\n",
    "headers_get = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Headers for POST requests (sending data)\n",
    "headers_post = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Check Available Providers\n",
    "\n",
    "First, let's verify that our TRL provider is properly configured and available. This endpoint will show us all the providers that are currently registered with the server, including their configuration and health status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'api': 'post_training', 'provider_id': 'trl', 'provider_type': 'inline::trl', 'config': {'device': 'cpu', 'dpo_beta': 0.1, 'use_reference_model': True, 'max_seq_length': 2048, 'gradient_checkpointing': False, 'logging_steps': 10, 'warmup_ratio': 0.1, 'weight_decay': 0.01}, 'health': {'status': 'Not Implemented', 'message': 'Provider does not implement health check'}}, {'api': 'datasetio', 'provider_id': 'localfs', 'provider_type': 'inline::localfs', 'config': {'kvstore': {'type': 'sqlite', 'db_path': '/tmp/llama_stack_provider_trl/datasetio.db'}}, 'health': {'status': 'Not Implemented', 'message': 'Provider does not implement health check'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Get the list of available providers\n",
    "# This will show us what services are available (TRL for post-training, localfs for datasets, etc.)\n",
    "\n",
    "url_providers = f\"{base_url}/v1/providers\"\n",
    "response_providers = requests.get(url_providers, headers=headers_get)\n",
    "\n",
    "# Display the providers and their configurations\n",
    "# You should see 'trl' provider for post-training and 'localfs' for dataset storage\n",
    "print(response_providers.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Working with Datasets\n",
    "\n",
    "For DPO training, we need preference datasets that contain prompts with both \"chosen\" (preferred) and \"rejected\" responses. Let's first check what datasets are currently available in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': []}\n"
     ]
    }
   ],
   "source": [
    "# List all available datasets in the system\n",
    "# This will show existing datasets that can be used for training\n",
    "\n",
    "url_datasets = f\"{base_url}/v1/datasets\"\n",
    "response_datasets = requests.get(url_datasets, headers=headers_get)\n",
    "\n",
    "# Display the datasets - each dataset should have a purpose (e.g., 'post-training/messages')\n",
    "# and a source containing the training data\n",
    "print(response_datasets.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.1 Upload a DPO Dataset\n",
    "\n",
    "Now let's create and upload a preference dataset for DPO training. This dataset contains examples of prompts with both \"chosen\" (high-quality) and \"rejected\" (low-quality) responses. The model will learn to prefer the chosen responses over the rejected ones.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `prompt`: The input question or instruction\n",
    "- `chosen`: The preferred/high-quality response\n",
    "- `rejected`: The less preferred/low-quality response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Upload Status: 200\n",
      "Dataset Upload Response: {'identifier': 'test-dpo-dataset-inline-large', 'provider_resource_id': 'test-dpo-dataset-inline-large', 'provider_id': 'localfs', 'type': 'dataset', 'owner': {'principal': '', 'attributes': {}}, 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff with data.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain the concept of fine-tuning', 'chosen': 'Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.', 'rejected': 'Fine-tuning means making a model better by training it more.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Inline DPO preference training dataset'}}\n"
     ]
    }
   ],
   "source": [
    "# Upload a DPO (Direct Preference Optimization) dataset\n",
    "# This creates a preference dataset with example prompt-response pairs\n",
    "\n",
    "url_upload_dataset = f\"{base_url}/v1/datasets\"\n",
    "\n",
    "# Define the dataset payload with preference pairs\n",
    "dataset_payload = {\n",
    "    \"dataset_id\": \"test-dpo-dataset-inline-large\",  #\n",
    "    \"purpose\": \"post-training/messages\",             \n",
    "    \"dataset_type\": \"preference\",                    \n",
    "    \"source\": {\n",
    "        \"type\": \"rows\",                              \n",
    "        \"rows\": [\n",
    "            {\n",
    "\n",
    "                \"prompt\": \"What is machine learning?\",\n",
    "                \"chosen\": \"Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.\",\n",
    "                \"rejected\": \"Machine learning is just computers doing math stuff with data.\"\n",
    "            },\n",
    "            {\n",
    "\n",
    "                \"prompt\": \"Write a hello world program\",\n",
    "                \"chosen\": \"Here is a simple hello world program in Python:\\n\\n```python\\nprint(\\\"Hello, World!\\\")\\n```\",\n",
    "                \"rejected\": \"print hello world\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": \"Explain the concept of fine-tuning\",\n",
    "                \"chosen\": \"Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.\",\n",
    "                \"rejected\": \"Fine-tuning means making a model better by training it more.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"provider_id\": \"localfs\",                    # Use local filesystem storage\n",
    "        \"description\": \"Inline DPO preference training dataset\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the POST request to upload the dataset\n",
    "response_dataset = requests.post(url_upload_dataset, headers=headers_post, json=dataset_payload)\n",
    "print(\"Dataset Upload Status:\", response_dataset.status_code)\n",
    "print(\"Dataset Upload Response:\", response_dataset.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Verify Dataset Upload\n",
    "\n",
    "Let's confirm that our dataset was successfully uploaded and is now available in the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'identifier': 'test-dpo-dataset-inline-large', 'provider_resource_id': 'test-dpo-dataset-inline-large', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff with data.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain the concept of fine-tuning', 'chosen': 'Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.', 'rejected': 'Fine-tuning means making a model better by training it more.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Inline DPO preference training dataset'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Verify that our dataset was successfully uploaded\n",
    "# This should now show our \"test-dpo-dataset-inline-large\" dataset\n",
    "\n",
    "url_datasets = f\"{base_url}/v1/datasets\"\n",
    "response_datasets = requests.get(url_datasets, headers=headers_get)\n",
    "\n",
    "# The response should include our uploaded dataset with all the preference pairs\n",
    "print(response_datasets.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Training a Model with DPO\n",
    "\n",
    "Now that we have our dataset ready, let's start a DPO training job. DPO (Direct Preference Optimization) is a method for training language models to align with human preferences without requiring a separate reward model.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `model`: The base model to fine-tune (e.g., granite-3.3-2b-base, distilgpt2)\n",
    "- `algorithm_config`: DPO-specific settings like beta, reward scaling\n",
    "- `training_config`: Standard training parameters like learning rate, epochs, batch size\n",
    "- `dataset_id`: The preference dataset we uploaded earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Model Status: 200\n",
      "Train Model Response: {'job_uuid': 'dpo-training-ibm-granite-3.3-2b-'}\n"
     ]
    }
   ],
   "source": [
    "# Train a model using DPO\n",
    "\n",
    "url_train_model = f\"{base_url}/v1/post-training/preference-optimize\"\n",
    "\n",
    "train_model_data = {\n",
    "    \"job_uuid\": \"dpo-training-ibm-granite-3.3-2b-\",\n",
    "    \"model\": \"ibm-granite/granite-3.3-2b-base\", # \"Larger = ibm-granite/granite-3.3-2b-base\" \"Smaller (Quick) = distilgpt2\"\n",
    "    \"finetuned_model\": \"dpo-ibm-granite-3.3-2b\",\n",
    "    \"checkpoint_dir\": \"./checkpoints\",\n",
    "    \"algorithm_config\": {\n",
    "        \"type\": \"dpo\", \n",
    "        \"reward_scale\": 1.0, \n",
    "        \"reward_clip\": 5.0, \n",
    "        \"epsilon\": 0.1,\n",
    "        \"gamma\": 0.99 \n",
    "    },\n",
    "    \"training_config\": {    \n",
    "        \"n_epochs\": 3,\n",
    "        \"max_steps_per_epoch\": 50,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"lr_scheduler_type\": \"constant\",\n",
    "        \"data_config\": {\n",
    "            \"dataset_id\": \"test-dpo-dataset-inline-large\",\n",
    "            \"batch_size\": 2,\n",
    "            \"shuffle\": True,\n",
    "            \"data_format\": \"instruct\",\n",
    "            \"train_split_percentage\": 0.8\n",
    "        }\n",
    "    },\n",
    "    \"hyperparam_search_config\": {},\n",
    "    \"logger_config\": {}\n",
    "}\n",
    "\n",
    "response_train_model = requests.post(url_train_model, headers=headers_post, json=train_model_data)\n",
    "print(\"Train Model Status:\", response_train_model.status_code)\n",
    "print(\"Train Model Response:\", response_train_model.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Monitoring Training Jobs\n",
    "\n",
    "Once a training job is started, you can monitor its progress and retrieve information about all jobs or specific jobs. Training jobs go through various states: `scheduled`, `in_progress`, `completed`, or `failed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'job_uuid': 'dpo-training-ibm-granite-3.3-2b-'}]}\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all post-training jobs\n",
    "# This will show all training jobs that have been submitted to the system\n",
    "\n",
    "url_post_training_jobs = f\"{base_url}/v1/post-training/jobs\"\n",
    "response_post_training_jobs = requests.get(url_post_training_jobs, headers=headers_get)\n",
    "\n",
    "# Display all jobs with their current status and metadata\n",
    "print(response_post_training_jobs.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.1 Check Specific Job Status\n",
    "\n",
    "You can get detailed information about a specific training job using its job UUID. This is useful for monitoring progress and checking when training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: 200\n",
      "Job Status Response: {'job_uuid': 'dpo-training-ibm-granite-3.3-2b-', 'status': 'in_progress', 'scheduled_at': '2025-06-11T15:08:26.509622Z', 'started_at': '2025-06-11T15:08:26.511107Z', 'completed_at': None, 'resources_allocated': None, 'checkpoints': []}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of a specific training job\n",
    "# Replace the job_uuid with the actual UUID from your training job\n",
    "\n",
    "job_uuid = \"dpo-training-ibm-granite-3.3-2b-\"  # The job UUID from the training request\n",
    "url_job_status = f\"{base_url}/v1/post-training/job/status?job_uuid={job_uuid}\"\n",
    "\n",
    "response_job_status = requests.get(url_job_status, headers=headers_get)\n",
    "\n",
    "print(\"Job Status:\", response_job_status.status_code)\n",
    "# The response will include: status, scheduled_at, started_at, completed_at, checkpoints\n",
    "print(\"Job Status Response:\", response_job_status.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.2 Retrieve Training Artifacts\n",
    "\n",
    "After training is complete (or during training), you can retrieve the artifacts generated by the job, including model checkpoints and training metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Artifacts Status: 200\n",
      "Job Artifacts Response: {'job_uuid': 'dpo-training-ibm-granite-3.3-2b-', 'checkpoints': [{'identifier': 'ibm-granite/granite-3.3-2b-base-dpo-3', 'created_at': '2025-06-11T15:19:00.788615Z', 'epoch': 3, 'post_training_job_id': 'dpo-training-ibm-granite-3.3-2b-', 'path': 'checkpoints/dpo_model', 'training_metrics': None}]}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve artifacts (checkpoints, metrics) from a completed training job\n",
    "# This will show available model checkpoints and their metadata\n",
    "\n",
    "url_job_artifacts = f\"{base_url}/v1/post-training/job/artifacts?job_uuid={job_uuid}\"\n",
    "response_job_artifacts = requests.get(url_job_artifacts, headers=headers_get)\n",
    "\n",
    "print(\"Job Artifacts Status:\", response_job_artifacts.status_code)\n",
    "# The response will include checkpoint information: identifier, path, epoch, training_metrics\n",
    "print(\"Job Artifacts Response:\", response_job_artifacts.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for using the Llama Stack TRL provider:\n",
    "\n",
    "1. **Setup**: Configure API endpoints and authentication\n",
    "2. **Provider Verification**: Check that TRL and localfs providers are available\n",
    "3. **Dataset Management**: Upload preference datasets for DPO training\n",
    "4. **Model Training**: Start DPO training jobs with custom configurations\n",
    "5. **Job Monitoring**: Track training progress and status\n",
    "6. **Artifact Retrieval**: Access trained model checkpoints and metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Use the trained model**: Load the checkpoints from the specified checkpoint directory\n",
    "- **Experiment with parameters**: Try different DPO configurations (beta, learning rates, etc.)\n",
    "- **Scale up**: Use larger datasets and models for production scenarios\n",
    "- **Integration**: Integrate the trained models into your applications\n",
    "\n",
    "### Troubleshooting Tips\n",
    "\n",
    "- **Job already exists error**: Use a unique `job_uuid` for each training run\n",
    "- **Empty job list**: Jobs might be cleaned up after completion; check artifacts instead\n",
    "- **Training failures**: Monitor logs and adjust batch size or learning rate if needed\n",
    "- **Memory issues**: Consider using smaller models like `distilgpt2` for testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
