{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# How to Use Llama Stack TRL Remote Provider\n",
    "\n",
    "This notebook demonstrates how to interact with the Llama Stack TRL Remote Provider for post-training tasks, specifically Direct Preference Optimization (DPO). The remote provider allows you to run training jobs on a separate service while managing them through the Llama Stack API.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The TRL Remote Provider enables you to:\n",
    "- Upload preference datasets for remote training\n",
    "- Submit DPO training jobs to remote TRL service\n",
    "- Monitor remote training progress and retrieve artifacts\n",
    "- Manage distributed post-training workflows through a REST API\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure that:\n",
    "1. The TRL Remote Service is running on `http://localhost:8080`\n",
    "2. The Llama Stack client is running on `http://127.0.0.1:8321`\n",
    "3. You have the required dependencies installed\n",
    "4. The client is configured with the remote TRL provider\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by importing the required libraries and setting up our API configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "\n",
    "# Base URL for the Llama Stack client API\n",
    "# The client connects to the remote TRL service for training\n",
    "base_url = \"http://127.0.0.1:8321\"\n",
    "\n",
    "# Remote TRL service runs on http://localhost:8080\n",
    "# Client forwards training requests to the remote service\n",
    "remote_service_url = \"http://localhost:8080\"\n",
    "\n",
    "# Headers for GET requests (retrieving data)\n",
    "headers_get = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Headers for POST requests (sending data)\n",
    "headers_post = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Check Available Providers\n",
    "\n",
    "First, let's verify that our remote TRL provider is properly configured and available. This endpoint will show us all the providers that are currently registered with the Llama Stack client, including the remote TRL provider configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'api': 'post_training', 'provider_id': 'trl_remote', 'provider_type': 'remote::trl', 'config': {'base_url': 'http://localhost:8080', 'timeout': 3600, 'connect_timeout': 30, 'max_retries': 3, 'retry_delay': 5, 'training_config': {'device': 'cuda', 'dpo_beta': 0.1, 'use_reference_model': True, 'max_seq_length': 2048, 'gradient_checkpointing': False, 'logging_steps': 10, 'warmup_ratio': 0.1, 'weight_decay': 0.01}}, 'health': {'status': 'Not Implemented', 'message': 'Provider does not implement health check'}}, {'api': 'datasetio', 'provider_id': 'localfs', 'provider_type': 'inline::localfs', 'config': {'kvstore': {'type': 'sqlite', 'db_path': '/tmp/llama_stack_provider_trl_remote/datasetio.db'}}, 'health': {'status': 'Not Implemented', 'message': 'Provider does not implement health check'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Get the list of available providers\n",
    "# This will show us what services are available (remote TRL for post-training, localfs for datasets, etc.)\n",
    "\n",
    "url_providers = f\"{base_url}/v1/providers\"\n",
    "response_providers = requests.get(url_providers, headers=headers_get)\n",
    "\n",
    "# Display the providers and their configurations\n",
    "# You should see 'remote::trl' provider for post-training and 'localfs' for dataset storage\n",
    "print(response_providers.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Working with Datasets\n",
    "\n",
    "For remote DPO training, we need preference datasets that contain prompts with both \"chosen\" (preferred) and \"rejected\" responses. Datasets are stored locally but sent to the remote training service during job execution. Let's first check what datasets are currently available in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'identifier': 'dataset-181c39ce-d135-48dc-86d8-158cfe7d231b', 'provider_resource_id': 'dataset-181c39ce-d135-48dc-86d8-158cfe7d231b', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is 2+2?', 'chosen': '2+2 equals 4. This is basic arithmetic.', 'rejected': 'I dont know math.'}, {'prompt': 'What is the capital of France?', 'chosen': 'Paris is the capital city of France.', 'rejected': 'Dunno.'}, {'prompt': 'What is artificial intelligence?', 'chosen': 'AI is the simulation of human intelligence by machines.', 'rejected': 'No idea what that is.'}, {'prompt': 'What color is the sky?', 'chosen': 'The sky appears blue during clear weather.', 'rejected': 'I dont know colors.'}, {'prompt': 'What is the largest planet?', 'chosen': 'Jupiter is the largest planet in our solar system.', 'rejected': 'Not sure about planets.'}, {'prompt': 'Who wrote Hamlet?', 'chosen': 'William Shakespeare wrote Hamlet.', 'rejected': 'I dont know literature.'}, {'prompt': 'What is water made of?', 'chosen': 'Water is composed of hydrogen and oxygen (H2O).', 'rejected': 'Not sure about chemistry.'}, {'prompt': 'What is the speed of light?', 'chosen': 'Light travels at approximately 299,792,458 meters per second.', 'rejected': 'I dont know physics.'}, {'prompt': 'What is gravity?', 'chosen': 'Gravity is the force that attracts objects toward each other.', 'rejected': 'No clue about forces.'}, {'prompt': 'What is the smallest unit of matter?', 'chosen': 'Atoms are the basic units of matter.', 'rejected': 'Dunno about matter.'}, {'prompt': 'How many continents are there?', 'chosen': 'There are seven continents on Earth.', 'rejected': 'I dont know geography.'}, {'prompt': 'What is photosynthesis?', 'chosen': 'Photosynthesis is how plants convert sunlight into energy.', 'rejected': 'Not sure about plants.'}]}, 'metadata': {}}, {'identifier': 'remote-dpo-test', 'provider_resource_id': 'remote-dpo-test', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is the capital of France?', 'chosen': 'Paris is the capital of France.', 'rejected': 'I dont know.'}, {'prompt': 'What is 2+2?', 'chosen': '2+2 equals 4.', 'rejected': 'I dont know math.'}]}, 'metadata': {}}, {'identifier': 'remote-dpo-test-dataset', 'provider_resource_id': 'remote-dpo-test-dataset', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is the capital of France?', 'chosen': \"The capital of France is Paris. Paris is the largest city in France and serves as the country's political, economic, and cultural center.\", 'rejected': 'France capital is Paris.'}, {'prompt': 'Explain machine learning briefly', 'chosen': 'Machine learning is a subset of artificial intelligence where algorithms learn patterns from data to make predictions or decisions without being explicitly programmed for each specific task.', 'rejected': 'ML is when computers learn stuff from data.'}, {'prompt': 'Write a simple Python function to add two numbers', 'chosen': 'Here\\'s a simple Python function to add two numbers:\\n\\n```python\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers and return the result.\\n    \\n    Args:\\n        a: First number\\n        b: Second number\\n    \\n    Returns:\\n        Sum of a and b\\n    \"\"\"\\n    return a + b\\n```', 'rejected': 'def add(x,y): return x+y'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Remote TRL DPO test dataset'}}, {'identifier': 'test-dpo-dataset-distilgpt2', 'provider_resource_id': 'test-dpo-dataset-distilgpt2', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff with data.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain the concept of fine-tuning', 'chosen': 'Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.', 'rejected': 'Fine-tuning means making a model better by training it more.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Inline DPO preference training dataset for DistilGPT-2'}}, {'identifier': 'test-dpo-dataset-remote', 'provider_resource_id': 'test-dpo-dataset-remote', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff with data.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain the concept of fine-tuning', 'chosen': 'Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.', 'rejected': 'Fine-tuning means making a model better by training it more.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Remote DPO preference training dataset'}}, {'identifier': 'test-dpo-dataset-remote-50', 'provider_resource_id': 'test-dpo-dataset-remote-50', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of AI where algorithms learn from data to make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain fine-tuning', 'chosen': 'Fine-tuning adapts a pre-trained model to a specific task by additional training on task-specific data.', 'rejected': 'Fine-tuning is training more.'}, {'prompt': 'What is deep learning?', 'chosen': 'Deep learning is a subset of machine learning using neural networks with multiple layers to learn complex patterns.', 'rejected': 'Deep learning is big neural networks.'}, {'prompt': 'Explain supervised learning', 'chosen': 'Supervised learning trains models using labeled datasets where the correct output is provided for each input.', 'rejected': 'Supervised learning means learning with supervision.'}, {'prompt': 'What is unsupervised learning?', 'chosen': 'Unsupervised learning involves training on unlabeled data to discover hidden structures or patterns.', 'rejected': 'Unsupervised learning has no teacher.'}, {'prompt': 'Define reinforcement learning', 'chosen': 'Reinforcement learning trains agents through trial and error by rewarding desirable actions and penalizing mistakes.', 'rejected': 'Reinforcement learning is learning by doing stuff.'}, {'prompt': 'Explain overfitting', 'chosen': 'Overfitting occurs when a model learns the training data too well, including its noise, reducing its generalization on new data.', 'rejected': 'Overfitting means memorizing the data.'}, {'prompt': 'What is natural language processing?', 'chosen': 'Natural language processing enables computers to understand, interpret, and generate human language.', 'rejected': 'NLP is about computers talking.'}, {'prompt': 'Explain gradient descent', 'chosen': 'Gradient descent is an optimization algorithm that iteratively updates model parameters to minimize loss by moving in the direction of the negative gradient.', 'rejected': 'Gradient descent moves the model slowly to better values.'}, {'prompt': 'Explain transfer learning', 'chosen': 'Transfer learning leverages knowledge from a pre-trained model to solve a related task with less data and computation.', 'rejected': 'Transfer learning copies models to new tasks.'}, {'prompt': 'What is computer vision?', 'chosen': 'Computer vision allows machines to interpret and analyze visual information from the world.', 'rejected': 'Computer vision is machines looking at stuff.'}, {'prompt': 'What is artificial general intelligence?', 'chosen': 'Artificial general intelligence refers to highly autonomous systems that can outperform humans at most economically valuable work.', 'rejected': 'AGI is AI that can do everything.'}, {'prompt': 'What is data augmentation?', 'chosen': 'Data augmentation artificially increases the size of training data by generating modified versions of existing data.', 'rejected': 'Data augmentation makes more data.'}, {'prompt': 'Explain tokenization', 'chosen': 'Tokenization is the process of breaking text into smaller units, such as words or subwords, to prepare it for model processing.', 'rejected': 'Tokenization cuts up sentences.'}, {'prompt': 'Explain large language models', 'chosen': 'Large language models are trained on massive datasets and can generate coherent text and understand complex language patterns.', 'rejected': 'LLMs are just huge models that write stuff.'}, {'prompt': 'What is a transformer model?', 'chosen': 'Transformer models use self-attention mechanisms to process input sequences in parallel, enabling efficient and scalable learning.', 'rejected': 'Transformers are big models that pay attention.'}, {'prompt': 'Explain self-attention', 'chosen': 'Self-attention allows a model to weigh different parts of the input sequence when processing a token, enabling better context understanding.', 'rejected': 'Self-attention looks at itself.'}, {'prompt': 'What is zero-shot learning?', 'chosen': \"Zero-shot learning allows models to handle tasks they weren't explicitly trained on by leveraging general knowledge.\", 'rejected': 'Zero-shot means no training needed.'}, {'prompt': 'What is prompt engineering?', 'chosen': 'Prompt engineering involves crafting input prompts to elicit desired responses from language models.', 'rejected': 'Prompt engineering is asking the model stuff.'}, {'prompt': 'Explain hallucination in LLMs', 'chosen': 'Hallucination refers to language models generating plausible-sounding but factually incorrect or fabricated outputs.', 'rejected': 'Hallucination is making up stuff.'}, {'prompt': 'What is alignment in AI?', 'chosen': 'AI alignment ensures models behave in ways that match human values and intentions.', 'rejected': 'Alignment is making AI not dangerous.'}, {'prompt': 'What is multi-modal AI?', 'chosen': 'Multi-modal AI can process and reason across multiple types of data like text, images, audio, and video.', 'rejected': 'Multi-modal AI does many things at once.'}, {'prompt': 'Explain RAG (retrieval-augmented generation)', 'chosen': 'RAG combines retrieval from knowledge bases with generative models to improve factual accuracy in outputs.', 'rejected': 'RAG gets facts from search.'}, {'prompt': 'What is LoRA?', 'chosen': 'LoRA is a fine-tuning technique that adds trainable low-rank matrices to pre-trained model weights, enabling efficient adaptation.', 'rejected': 'LoRA is small changes to big models.'}, {'prompt': 'What is quantization?', 'chosen': 'Quantization reduces model size and speeds up inference by representing weights with lower-precision numbers.', 'rejected': 'Quantization makes models smaller.'}, {'prompt': 'What is model distillation?', 'chosen': 'Distillation transfers knowledge from a large model to a smaller one, preserving performance while reducing size and computation.', 'rejected': 'Distillation is shrinking models.'}, {'prompt': 'Explain chain-of-thought prompting', 'chosen': 'Chain-of-thought prompting encourages models to explain reasoning steps before answering, improving accuracy on complex tasks.', 'rejected': 'It makes the model think out loud.'}, {'prompt': 'Explain temperature in language models', 'chosen': 'Temperature controls randomness in model output â€” higher values generate more diverse responses, lower values make outputs more deterministic.', 'rejected': 'Temperature makes answers more random.'}, {'prompt': 'What is top-k sampling?', 'chosen': 'Top-k sampling limits token selection to the k most probable tokens, reducing randomness and improving output quality.', 'rejected': 'Top-k picks from the best options.'}, {'prompt': 'What is beam search?', 'chosen': 'Beam search explores multiple output sequences simultaneously, selecting the most probable complete sequence.', 'rejected': 'Beam search tries several paths.'}, {'prompt': 'What is token limit?', 'chosen': 'Token limit refers to the maximum number of tokens a model can process in a single input or output sequence.', 'rejected': 'Token limit is how long the model can read.'}, {'prompt': 'What is RLHF?', 'chosen': 'Reinforcement Learning with Human Feedback fine-tunes models by optimizing based on human preference scores.', 'rejected': 'RLHF is when humans give feedback.'}, {'prompt': 'Explain pre-training vs fine-tuning', 'chosen': 'Pre-training involves general unsupervised learning on large corpora; fine-tuning adapts models to specific tasks using supervised data.', 'rejected': 'Pre-training is first, fine-tuning is second.'}, {'prompt': 'What is a reward model?', 'chosen': 'A reward model predicts the desirability of model outputs and guides optimization during preference-based training.', 'rejected': 'Reward model gives scores.'}, {'prompt': 'What is synthetic data?', 'chosen': 'Synthetic data is artificially generated data that mimics real-world data, often used to augment training datasets.', 'rejected': 'Synthetic data is fake data.'}, {'prompt': 'Explain few-shot learning', 'chosen': 'Few-shot learning enables models to generalize to new tasks with very few examples by leveraging prior knowledge.', 'rejected': 'Few-shot is learning from a few examples.'}, {'prompt': 'What is model drift?', 'chosen': \"Model drift occurs when a model's performance degrades over time due to changes in data distributions.\", 'rejected': 'Model drift means the model gets worse.'}, {'prompt': 'Explain evaluation benchmarks', 'chosen': 'Evaluation benchmarks provide standardized tasks to assess model performance across various domains.', 'rejected': 'Benchmarks test models.'}, {'prompt': 'What is instruction tuning?', 'chosen': 'Instruction tuning fine-tunes models to follow explicit natural language instructions for better task generalization.', 'rejected': 'Instruction tuning teaches models to follow commands.'}, {'prompt': 'Explain safety in LLM deployment', 'chosen': 'Safety in LLM deployment involves techniques to prevent harmful, biased, or dangerous outputs.', 'rejected': 'Safety means not letting AI say bad stuff.'}, {'prompt': 'What is multi-turn dialogue?', 'chosen': 'Multi-turn dialogue refers to conversations where the model maintains context across multiple user interactions.', 'rejected': 'Multi-turn is talking back and forth.'}, {'prompt': 'What is hallucination detection?', 'chosen': 'Hallucination detection methods aim to identify when a model generates factually incorrect outputs.', 'rejected': 'It spots made up stuff.'}, {'prompt': 'What is model alignment research?', 'chosen': 'Model alignment research focuses on ensuring AI systems behave consistently with human values and goals.', 'rejected': 'Alignment research makes AI safe.'}, {'prompt': 'What is model interpretability?', 'chosen': 'Interpretability involves techniques that make model decisions understandable to humans.', 'rejected': 'Interpretability explains what AI is doing.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Expanded DPO preference dataset with 50 examples'}}]}\n"
     ]
    }
   ],
   "source": [
    "# List all available datasets in the system\n",
    "# This will show existing datasets that can be used for training\n",
    "\n",
    "url_datasets = f\"{base_url}/v1/datasets\"\n",
    "response_datasets = requests.get(url_datasets, headers=headers_get)\n",
    "\n",
    "# Display the datasets - each dataset should have a purpose (e.g., 'post-training/messages')\n",
    "# and a source containing the training data\n",
    "print(response_datasets.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.1 Upload a DPO Dataset\n",
    "\n",
    "Now let's create and upload a preference dataset for remote DPO training. This dataset contains examples of prompts with both \"chosen\" (high-quality) and \"rejected\" (low-quality) responses. When training starts, this dataset will be sent to the remote TRL service for processing.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `prompt`: The input question or instruction\n",
    "- `chosen`: The preferred/high-quality response\n",
    "- `rejected`: The less preferred/low-quality response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Upload Status: 200\n",
      "Dataset Upload Response: {'identifier': 'test-dpo-dataset-remote-final', 'provider_resource_id': 'test-dpo-dataset-remote-final', 'provider_id': 'localfs', 'type': 'dataset', 'owner': {'principal': '', 'attributes': {}}, 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is 2+2?', 'chosen': '2+2 equals 4. This is basic arithmetic.', 'rejected': \"I don't know math.\"}, {'prompt': 'What color is the sky?', 'chosen': 'The sky is blue during clear weather.', 'rejected': 'No idea about colors.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Minimal DPO dataset for fast testing'}}\n"
     ]
    }
   ],
   "source": [
    "# Upload a MINIMAL DPO dataset for fast remote training\n",
    "# This creates a tiny preference dataset for rapid testing\n",
    "\n",
    "url_upload_dataset = f\"{base_url}/v1/datasets\"\n",
    "\n",
    "# Define the minimal dataset payload - just 2 examples for speed!\n",
    "dataset_payload = {\n",
    "    \"dataset_id\": \"test-dpo-dataset-remote-final\",\n",
    "    \"purpose\": \"post-training/messages\",             \n",
    "    \"dataset_type\": \"preference\",                    \n",
    "    \"source\": {\n",
    "        \"type\": \"rows\",                              \n",
    "        \"rows\": [\n",
    "            {\n",
    "                \"prompt\": \"What is 2+2?\",\n",
    "                \"chosen\": \"2+2 equals 4. This is basic arithmetic.\",\n",
    "                \"rejected\": \"I don't know math.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": \"What color is the sky?\",\n",
    "                \"chosen\": \"The sky is blue during clear weather.\",\n",
    "                \"rejected\": \"No idea about colors.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"provider_id\": \"localfs\",                    # Use local filesystem storage\n",
    "        \"description\": \"Minimal DPO dataset for fast testing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the POST request to upload the dataset\n",
    "response_dataset = requests.post(url_upload_dataset, headers=headers_post, json=dataset_payload)\n",
    "print(\"Dataset Upload Status:\", response_dataset.status_code)\n",
    "print(\"Dataset Upload Response:\", response_dataset.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Verify Dataset Upload\n",
    "\n",
    "Let's confirm that our dataset was successfully uploaded and is now available in the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'identifier': 'dataset-181c39ce-d135-48dc-86d8-158cfe7d231b', 'provider_resource_id': 'dataset-181c39ce-d135-48dc-86d8-158cfe7d231b', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is 2+2?', 'chosen': '2+2 equals 4. This is basic arithmetic.', 'rejected': 'I dont know math.'}, {'prompt': 'What is the capital of France?', 'chosen': 'Paris is the capital city of France.', 'rejected': 'Dunno.'}, {'prompt': 'What is artificial intelligence?', 'chosen': 'AI is the simulation of human intelligence by machines.', 'rejected': 'No idea what that is.'}, {'prompt': 'What color is the sky?', 'chosen': 'The sky appears blue during clear weather.', 'rejected': 'I dont know colors.'}, {'prompt': 'What is the largest planet?', 'chosen': 'Jupiter is the largest planet in our solar system.', 'rejected': 'Not sure about planets.'}, {'prompt': 'Who wrote Hamlet?', 'chosen': 'William Shakespeare wrote Hamlet.', 'rejected': 'I dont know literature.'}, {'prompt': 'What is water made of?', 'chosen': 'Water is composed of hydrogen and oxygen (H2O).', 'rejected': 'Not sure about chemistry.'}, {'prompt': 'What is the speed of light?', 'chosen': 'Light travels at approximately 299,792,458 meters per second.', 'rejected': 'I dont know physics.'}, {'prompt': 'What is gravity?', 'chosen': 'Gravity is the force that attracts objects toward each other.', 'rejected': 'No clue about forces.'}, {'prompt': 'What is the smallest unit of matter?', 'chosen': 'Atoms are the basic units of matter.', 'rejected': 'Dunno about matter.'}, {'prompt': 'How many continents are there?', 'chosen': 'There are seven continents on Earth.', 'rejected': 'I dont know geography.'}, {'prompt': 'What is photosynthesis?', 'chosen': 'Photosynthesis is how plants convert sunlight into energy.', 'rejected': 'Not sure about plants.'}]}, 'metadata': {}}, {'identifier': 'remote-dpo-test', 'provider_resource_id': 'remote-dpo-test', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is the capital of France?', 'chosen': 'Paris is the capital of France.', 'rejected': 'I dont know.'}, {'prompt': 'What is 2+2?', 'chosen': '2+2 equals 4.', 'rejected': 'I dont know math.'}]}, 'metadata': {}}, {'identifier': 'remote-dpo-test-dataset', 'provider_resource_id': 'remote-dpo-test-dataset', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is the capital of France?', 'chosen': \"The capital of France is Paris. Paris is the largest city in France and serves as the country's political, economic, and cultural center.\", 'rejected': 'France capital is Paris.'}, {'prompt': 'Explain machine learning briefly', 'chosen': 'Machine learning is a subset of artificial intelligence where algorithms learn patterns from data to make predictions or decisions without being explicitly programmed for each specific task.', 'rejected': 'ML is when computers learn stuff from data.'}, {'prompt': 'Write a simple Python function to add two numbers', 'chosen': 'Here\\'s a simple Python function to add two numbers:\\n\\n```python\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers and return the result.\\n    \\n    Args:\\n        a: First number\\n        b: Second number\\n    \\n    Returns:\\n        Sum of a and b\\n    \"\"\"\\n    return a + b\\n```', 'rejected': 'def add(x,y): return x+y'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Remote TRL DPO test dataset'}}, {'identifier': 'test-dpo-dataset-distilgpt2', 'provider_resource_id': 'test-dpo-dataset-distilgpt2', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff with data.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain the concept of fine-tuning', 'chosen': 'Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.', 'rejected': 'Fine-tuning means making a model better by training it more.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Inline DPO preference training dataset for DistilGPT-2'}}, {'identifier': 'test-dpo-dataset-remote', 'provider_resource_id': 'test-dpo-dataset-remote', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It uses algorithms to find patterns in data and make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff with data.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain the concept of fine-tuning', 'chosen': 'Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain while leveraging its existing knowledge. This approach is more efficient than training from scratch.', 'rejected': 'Fine-tuning means making a model better by training it more.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Remote DPO preference training dataset'}}, {'identifier': 'test-dpo-dataset-remote-50', 'provider_resource_id': 'test-dpo-dataset-remote-50', 'provider_id': 'localfs', 'type': 'dataset', 'purpose': 'post-training/messages', 'source': {'type': 'rows', 'rows': [{'prompt': 'What is machine learning?', 'chosen': 'Machine learning is a branch of AI where algorithms learn from data to make predictions or decisions.', 'rejected': 'Machine learning is just computers doing math stuff.'}, {'prompt': 'Write a hello world program', 'chosen': 'Here is a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, World!\")\\n```', 'rejected': 'print hello world'}, {'prompt': 'Explain fine-tuning', 'chosen': 'Fine-tuning adapts a pre-trained model to a specific task by additional training on task-specific data.', 'rejected': 'Fine-tuning is training more.'}, {'prompt': 'What is deep learning?', 'chosen': 'Deep learning is a subset of machine learning using neural networks with multiple layers to learn complex patterns.', 'rejected': 'Deep learning is big neural networks.'}, {'prompt': 'Explain supervised learning', 'chosen': 'Supervised learning trains models using labeled datasets where the correct output is provided for each input.', 'rejected': 'Supervised learning means learning with supervision.'}, {'prompt': 'What is unsupervised learning?', 'chosen': 'Unsupervised learning involves training on unlabeled data to discover hidden structures or patterns.', 'rejected': 'Unsupervised learning has no teacher.'}, {'prompt': 'Define reinforcement learning', 'chosen': 'Reinforcement learning trains agents through trial and error by rewarding desirable actions and penalizing mistakes.', 'rejected': 'Reinforcement learning is learning by doing stuff.'}, {'prompt': 'Explain overfitting', 'chosen': 'Overfitting occurs when a model learns the training data too well, including its noise, reducing its generalization on new data.', 'rejected': 'Overfitting means memorizing the data.'}, {'prompt': 'What is natural language processing?', 'chosen': 'Natural language processing enables computers to understand, interpret, and generate human language.', 'rejected': 'NLP is about computers talking.'}, {'prompt': 'Explain gradient descent', 'chosen': 'Gradient descent is an optimization algorithm that iteratively updates model parameters to minimize loss by moving in the direction of the negative gradient.', 'rejected': 'Gradient descent moves the model slowly to better values.'}, {'prompt': 'Explain transfer learning', 'chosen': 'Transfer learning leverages knowledge from a pre-trained model to solve a related task with less data and computation.', 'rejected': 'Transfer learning copies models to new tasks.'}, {'prompt': 'What is computer vision?', 'chosen': 'Computer vision allows machines to interpret and analyze visual information from the world.', 'rejected': 'Computer vision is machines looking at stuff.'}, {'prompt': 'What is artificial general intelligence?', 'chosen': 'Artificial general intelligence refers to highly autonomous systems that can outperform humans at most economically valuable work.', 'rejected': 'AGI is AI that can do everything.'}, {'prompt': 'What is data augmentation?', 'chosen': 'Data augmentation artificially increases the size of training data by generating modified versions of existing data.', 'rejected': 'Data augmentation makes more data.'}, {'prompt': 'Explain tokenization', 'chosen': 'Tokenization is the process of breaking text into smaller units, such as words or subwords, to prepare it for model processing.', 'rejected': 'Tokenization cuts up sentences.'}, {'prompt': 'Explain large language models', 'chosen': 'Large language models are trained on massive datasets and can generate coherent text and understand complex language patterns.', 'rejected': 'LLMs are just huge models that write stuff.'}, {'prompt': 'What is a transformer model?', 'chosen': 'Transformer models use self-attention mechanisms to process input sequences in parallel, enabling efficient and scalable learning.', 'rejected': 'Transformers are big models that pay attention.'}, {'prompt': 'Explain self-attention', 'chosen': 'Self-attention allows a model to weigh different parts of the input sequence when processing a token, enabling better context understanding.', 'rejected': 'Self-attention looks at itself.'}, {'prompt': 'What is zero-shot learning?', 'chosen': \"Zero-shot learning allows models to handle tasks they weren't explicitly trained on by leveraging general knowledge.\", 'rejected': 'Zero-shot means no training needed.'}, {'prompt': 'What is prompt engineering?', 'chosen': 'Prompt engineering involves crafting input prompts to elicit desired responses from language models.', 'rejected': 'Prompt engineering is asking the model stuff.'}, {'prompt': 'Explain hallucination in LLMs', 'chosen': 'Hallucination refers to language models generating plausible-sounding but factually incorrect or fabricated outputs.', 'rejected': 'Hallucination is making up stuff.'}, {'prompt': 'What is alignment in AI?', 'chosen': 'AI alignment ensures models behave in ways that match human values and intentions.', 'rejected': 'Alignment is making AI not dangerous.'}, {'prompt': 'What is multi-modal AI?', 'chosen': 'Multi-modal AI can process and reason across multiple types of data like text, images, audio, and video.', 'rejected': 'Multi-modal AI does many things at once.'}, {'prompt': 'Explain RAG (retrieval-augmented generation)', 'chosen': 'RAG combines retrieval from knowledge bases with generative models to improve factual accuracy in outputs.', 'rejected': 'RAG gets facts from search.'}, {'prompt': 'What is LoRA?', 'chosen': 'LoRA is a fine-tuning technique that adds trainable low-rank matrices to pre-trained model weights, enabling efficient adaptation.', 'rejected': 'LoRA is small changes to big models.'}, {'prompt': 'What is quantization?', 'chosen': 'Quantization reduces model size and speeds up inference by representing weights with lower-precision numbers.', 'rejected': 'Quantization makes models smaller.'}, {'prompt': 'What is model distillation?', 'chosen': 'Distillation transfers knowledge from a large model to a smaller one, preserving performance while reducing size and computation.', 'rejected': 'Distillation is shrinking models.'}, {'prompt': 'Explain chain-of-thought prompting', 'chosen': 'Chain-of-thought prompting encourages models to explain reasoning steps before answering, improving accuracy on complex tasks.', 'rejected': 'It makes the model think out loud.'}, {'prompt': 'Explain temperature in language models', 'chosen': 'Temperature controls randomness in model output â€” higher values generate more diverse responses, lower values make outputs more deterministic.', 'rejected': 'Temperature makes answers more random.'}, {'prompt': 'What is top-k sampling?', 'chosen': 'Top-k sampling limits token selection to the k most probable tokens, reducing randomness and improving output quality.', 'rejected': 'Top-k picks from the best options.'}, {'prompt': 'What is beam search?', 'chosen': 'Beam search explores multiple output sequences simultaneously, selecting the most probable complete sequence.', 'rejected': 'Beam search tries several paths.'}, {'prompt': 'What is token limit?', 'chosen': 'Token limit refers to the maximum number of tokens a model can process in a single input or output sequence.', 'rejected': 'Token limit is how long the model can read.'}, {'prompt': 'What is RLHF?', 'chosen': 'Reinforcement Learning with Human Feedback fine-tunes models by optimizing based on human preference scores.', 'rejected': 'RLHF is when humans give feedback.'}, {'prompt': 'Explain pre-training vs fine-tuning', 'chosen': 'Pre-training involves general unsupervised learning on large corpora; fine-tuning adapts models to specific tasks using supervised data.', 'rejected': 'Pre-training is first, fine-tuning is second.'}, {'prompt': 'What is a reward model?', 'chosen': 'A reward model predicts the desirability of model outputs and guides optimization during preference-based training.', 'rejected': 'Reward model gives scores.'}, {'prompt': 'What is synthetic data?', 'chosen': 'Synthetic data is artificially generated data that mimics real-world data, often used to augment training datasets.', 'rejected': 'Synthetic data is fake data.'}, {'prompt': 'Explain few-shot learning', 'chosen': 'Few-shot learning enables models to generalize to new tasks with very few examples by leveraging prior knowledge.', 'rejected': 'Few-shot is learning from a few examples.'}, {'prompt': 'What is model drift?', 'chosen': \"Model drift occurs when a model's performance degrades over time due to changes in data distributions.\", 'rejected': 'Model drift means the model gets worse.'}, {'prompt': 'Explain evaluation benchmarks', 'chosen': 'Evaluation benchmarks provide standardized tasks to assess model performance across various domains.', 'rejected': 'Benchmarks test models.'}, {'prompt': 'What is instruction tuning?', 'chosen': 'Instruction tuning fine-tunes models to follow explicit natural language instructions for better task generalization.', 'rejected': 'Instruction tuning teaches models to follow commands.'}, {'prompt': 'Explain safety in LLM deployment', 'chosen': 'Safety in LLM deployment involves techniques to prevent harmful, biased, or dangerous outputs.', 'rejected': 'Safety means not letting AI say bad stuff.'}, {'prompt': 'What is multi-turn dialogue?', 'chosen': 'Multi-turn dialogue refers to conversations where the model maintains context across multiple user interactions.', 'rejected': 'Multi-turn is talking back and forth.'}, {'prompt': 'What is hallucination detection?', 'chosen': 'Hallucination detection methods aim to identify when a model generates factually incorrect outputs.', 'rejected': 'It spots made up stuff.'}, {'prompt': 'What is model alignment research?', 'chosen': 'Model alignment research focuses on ensuring AI systems behave consistently with human values and goals.', 'rejected': 'Alignment research makes AI safe.'}, {'prompt': 'What is model interpretability?', 'chosen': 'Interpretability involves techniques that make model decisions understandable to humans.', 'rejected': 'Interpretability explains what AI is doing.'}]}, 'metadata': {'provider_id': 'localfs', 'description': 'Expanded DPO preference dataset with 50 examples'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Verify that our dataset was successfully uploaded\n",
    "# This should now show our \"test-dpo-dataset-inline-large\" dataset\n",
    "\n",
    "url_datasets = f\"{base_url}/v1/datasets\"\n",
    "response_datasets = requests.get(url_datasets, headers=headers_get)\n",
    "\n",
    "# The response should include our uploaded dataset with all the preference pairs\n",
    "print(response_datasets.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Submitting Remote DPO Training Jobs\n",
    "\n",
    "Now that we have our dataset ready, let's submit a DPO training job to the remote TRL service. The Llama Stack client will forward this request to the remote service running on port 8080, which will execute the actual training.\n",
    "\n",
    "**Key Parameters for Remote Training:**\n",
    "- `model`: The base model to fine-tune (e.g., granite-3.3-2b-base, distilgpt2)\n",
    "- `algorithm_config`: Sent as LoRA format (client validation) but converted to DPO by remote service\n",
    "- `training_config`: Standard training parameters like learning rate, epochs, batch size\n",
    "- `dataset_id`: The preference dataset that will be sent to the remote service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Status: 200\n",
      "Training Response: {'job_uuid': 'remote-dpo-fast-test-granite-final-final'}\n"
     ]
    }
   ],
   "source": [
    "# Submit FAST DPO training job to remote TRL service ðŸš€\n",
    "# MINIMAL CONFIG: Completes in ~30 seconds with 8-GPU FSDP!\n",
    "\n",
    "url_train_model = f\"{base_url}/v1/post-training/preference-optimize\"\n",
    "\n",
    "train_model_data = {\n",
    "    \"job_uuid\": \"remote-dpo-fast-test-granite-final-final\",  # Unique job ID\n",
    "    \"model\": \"ibm-granite/granite-3.3-2b-base\",  # Small 82M parameter model (fast to load)\n",
    "    \"finetuned_model\": \"dpo-granite-3.3-2b-base-fast-final-final\",\n",
    "    \"checkpoint_dir\": \"./checkpoints\",  # Save to absolute /checkpoints directory\n",
    "    # NOTE: Client requires LoRA format but remote service converts to DPO\n",
    "    \"algorithm_config\": {\n",
    "        \"type\": \"LoRA\",\n",
    "        \"lora_attn_modules\": [\"attn\"],\n",
    "        \"apply_lora_to_mlp\": False,\n",
    "        \"apply_lora_to_output\": False,\n",
    "        \"rank\": 8,   # Smaller rank = faster training\n",
    "        \"alpha\": 16  # Smaller alpha\n",
    "    },\n",
    "    \"training_config\": {    \n",
    "        \"n_epochs\": 1,                    # Just 1 epoch\n",
    "        \"max_steps_per_epoch\": 2,         # Only 2 steps total! \n",
    "        \"gradient_accumulation_steps\": 1, # No accumulation\n",
    "        \"data_config\": {\n",
    "            \"dataset_id\": \"test-dpo-dataset-remote-final\",\n",
    "            \"batch_size\": 1,              # Smallest batch size \n",
    "            \"shuffle\": False,             # Skip shuffling for speed\n",
    "            \"data_format\": \"instruct\"\n",
    "        },\n",
    "        \"optimizer_config\": {\n",
    "            \"optimizer_type\": \"adamw\",\n",
    "            \"lr\": 1e-4,                   # Higher LR for faster convergence\n",
    "            \"lr_scheduler_type\": \"constant\", # No scheduling overhead\n",
    "            \"warmup_steps\": 0,            # No warmup steps\n",
    "            \"weight_decay\": 0.0,          # No weight decay\n",
    "            \"num_warmup_steps\": 0\n",
    "        }\n",
    "    },\n",
    "    \"hyperparam_search_config\": {},\n",
    "    \"logger_config\": {}\n",
    "}\n",
    "\n",
    "# This should complete in ~30 seconds: Client -> Llama Stack -> Remote TRL Service\n",
    "response_train_model = requests.post(url_train_model, headers=headers_post, json=train_model_data)\n",
    "print(\"Training Status:\", response_train_model.status_code)\n",
    "print(\"Training Response:\", response_train_model.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Monitoring Remote Training Jobs\n",
    "\n",
    "Once a remote training job is submitted, you can monitor its progress through the Llama Stack client. The client communicates with the remote TRL service to get job status and artifacts. Training jobs go through various states: `scheduled`, `in_progress`, `completed`, or `failed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'job_uuid': 'remote-dpo-fast-test-granite-hacky'}]}\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all post-training jobs\n",
    "# This will show all training jobs that have been submitted to the system\n",
    "\n",
    "url_post_training_jobs = f\"{base_url}/v1/post-training/jobs\"\n",
    "response_post_training_jobs = requests.get(url_post_training_jobs, headers=headers_get)\n",
    "\n",
    "# Display all jobs with their current status and metadata\n",
    "print(response_post_training_jobs.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.1 Check Specific Job Status\n",
    "\n",
    "You can get detailed information about a specific training job using its job UUID. This is useful for monitoring progress and checking when training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: 200\n",
      "Job Status Response: {'job_uuid': 'remote-dpo-fast-test-granite-hacky', 'status': 'in_progress', 'scheduled_at': '2025-06-23T03:53:19.283760Z', 'started_at': '2025-06-23T03:53:19.285018Z', 'completed_at': None, 'resources_allocated': None, 'checkpoints': []}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of a specific training job\n",
    "# Replace the job_uuid with the actual UUID from your training job\n",
    "\n",
    "job_uuid = \"remote-dpo-fast-test-granite-hacky\"  # The job UUID from the remote training request\n",
    "url_job_status = f\"{base_url}/v1/post-training/job/status?job_uuid={job_uuid}\"\n",
    "\n",
    "response_job_status = requests.get(url_job_status, headers=headers_get)\n",
    "\n",
    "print(\"Job Status:\", response_job_status.status_code)\n",
    "# The response will include: status, scheduled_at, started_at, completed_at, checkpoints\n",
    "print(\"Job Status Response:\", response_job_status.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.2 Retrieve Training Artifacts\n",
    "\n",
    "After training is complete (or during training), you can retrieve the artifacts generated by the job, including model checkpoints and training metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Artifacts Status: 200\n",
      "Job Artifacts Response: {'job_uuid': 'remote-dpo-fast-test-granite-hacky', 'checkpoints': []}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve artifacts (checkpoints, metrics) from a completed training job\n",
    "# This will show available model checkpoints and their metadata\n",
    "\n",
    "url_job_artifacts = f\"{base_url}/v1/post-training/job/artifacts?job_uuid={job_uuid}\"\n",
    "response_job_artifacts = requests.get(url_job_artifacts, headers=headers_get)\n",
    "\n",
    "print(\"Job Artifacts Status:\", response_job_artifacts.status_code)\n",
    "# The response will include checkpoint information: identifier, path, epoch, training_metrics\n",
    "print(\"Job Artifacts Response:\", response_job_artifacts.json())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for using the Llama Stack TRL Remote Provider:\n",
    "\n",
    "1. **Setup**: Configure API endpoints for client and remote service\n",
    "2. **Provider Verification**: Check that remote TRL and localfs providers are available\n",
    "3. **Dataset Management**: Upload preference datasets for remote DPO training\n",
    "4. **Remote Training**: Submit DPO training jobs to remote TRL service\n",
    "5. **Job Monitoring**: Track remote training progress and status\n",
    "6. **Artifact Retrieval**: Access trained model checkpoints and metrics from remote service\n",
    "\n",
    "### Remote Provider Architecture\n",
    "\n",
    "```\n",
    "Client Request -> Llama Stack (8321) -> Remote TRL Service (8080) -> DPO Training\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
